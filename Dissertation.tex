\documentclass[11pt,a4paper]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=3.00cm]{geometry}
\usepackage{helvet}
\usepackage{natbib}
\renewcommand{\familydefault}{\sfdefault}

\title{Enhancing Social HRI using Affective Communication}
\author{Katie Winkle}

\begin{document}
\maketitle

\chapter{Introduction}

\chapter{Literature Review}

\section{IET and Emotional Expression in Humans}
\section{IET Effects}
- observed phenomena 
\section{Emotion Expression}
- body language and speech focus

\section{IET and Emotional Expression in Robots}
Giving robots the capability to display emotional expressions has been a recurrent theme in social robotics since the field's infancy \citep{breazeal1999build}. Recent demonstrations include robots whose emotional state and hence behaviour is adaptive based on such things at its `personality' \citep{park2009robot} or whether it is winning or losing at a game \citep{tielman2014adaptive}. Whilst adaptive emotional expression is certainly an area of further work for this project, the initial concern is simply the parameterised generation of pre-determined emotions which can be displayed through generic functional behaviours. Examples of such a framework have already been demonstrated and are clearly of particular relevance to this project (e.g. \citep{masuda2010motion}, \citep{lim2011converting}, \citep{xu2013mood}). Three such models, all inspired by the human  voice and motion parameterisation discussed in the previous subsection, have been identified for further discussion below.  

Masuda and Koto's system uses the six main parameters of LMA: space, time, weight, inclination, height and area, which are set based on previous analysis of observed movement emotion classification from a pilot experiment \citep{masuda2009emotion}. Implemented on a humanoid robot the resulting motion had an average emotion recognition rate greater than 60\% \citep{masuda2010motion}. Lim et al.'s framework for adding emotion to gesturing uses four parameters: speed, intensity, regularity and extent, which are set based on a mapping from the same features measured in an actor's speech sample. Implemented on a NAO the resulting motion had an emotion recognition rate of above 60\% and, when combined with the original speech sample, lead to improved recognition rates for the emotions of happiness and sadness compared to speech alone \citep{lim2011converting}. Xu et al.'s framework uses a combination of general motion and pose parameters (e.g. speed, decay rate, stroke curves) as well as gesture specific ones (e.g. palm up or down). In addition, the head is utilised as an effector which can be set in different poses. Parameter settings were then derived by averaging the results of an experiment in which participants were asked to set them in order to achieve specific emotional expressions on a NAO \citep{xu2013mood}. A later experiment demonstrated that different arousal and valence states can be recognised based on these parameters; however, no results for specific emotion recognition were described \citep{xu2013bodily}.

Considering these three models, Lim et al.'s is the most simple and yet achieved very similar recognition rates to Masuda and Masuda and Koto, and the idea of setting parameters based on feature matching to speech samples is attractive for generating natural looking behaviour. How well this feature mapping would work on artificially generated speech as opposed to actor samples would have to be investigated because the effectiveness of the whole system is dependent on how well the provided speech sample portrays emotion. However, if the parameters could be set in a different way, the concept of having a small number of generic parameters that map directly onto both speech and motion could form the basis of a simple yet powerful framework for easily generating complete emotional expressions. Xu et al.'s method of essentially crowd-sourcing parameter settings for different emotions could be explored as such an alternative option. Additionally, Xu et al.'s use of the head as a independent end effector for emotion generation, whilst not quite aligned to the key parameterisation concept, may be worth investigating. 

Of these three models only Xu et al. went on to evaluate the impact of their generated emotional expression on HRI \citep{xu2014robot}. [EXPLAIN WHAT HE FOUND! But not how - that will come in following section]

There have been other such studies designed to demonstrate the potential impact of emotional expression on HRI; however, they typically provide less detail on the mechanism for behaviour generation. One key objective of this project is to undertake experiment(s) in order to investigate the impact of affective communication on HRI, therefore the experimental design of relevant studies is worthwhile of discussion and is hence presented in the following section.

\section{Experimentally Evaluating the Impact of Affective Communication}
\subsection{HRI Studies}
- experimental detail of how any HRI studies described above, plus other less relevant HRI studies that may provide some inspiration
\subsection{[Additional i.e. Psychology/HCI] Studies}
- any human (or computer etc) studies which may also provide some inspiration for experiment design

\bibliography{MyReading}

\end{document}