\documentclass[11pt,a4paper]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=3.00cm]{geometry}
\usepackage{helvet}
\usepackage{natbib} %https://www.sharelatex.com/learn/Natbib_citation_styles
\renewcommand{\familydefault}{\sfdefault}

\title{Enhancing Social HRI using Affective Communication}
\author{Katie Winkle}

\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}

\chapter{Literature Review}
[intro paragraph]
\section{IET and Emotional Expression in Humans}

\subsection{IET and its Impact on Human Behaviour}
There are different hypotheses concerning the purpose of IET and emotional expression more generally in HHI. A functionalist approach (considering the consequences in order to determine the purpose) suggests that at the dyadic level, emotion expressions help individuals determine other's emotions, beliefs, intentions and orientation towards their relationship; and that evoking emotions in others is associated with behaviours such as avoidance, helping, affiliation and soothing \cite{keltner1999social}. An evolutionary approach (considering development over time and the link to population fitness) suggests that emotional expression evolved from being a physiological response (e.g. scrunching the nose to prevent inhalation of noxious gas) into a form of social communication, which observers evolved an ability to instantly and subconsciously decode in order to obtain information about the expresser and/or their environment \cite{shariff2011emotion}. Regardless of exact purpose, a consistent theme in psychology literature is the importance and subconscious nature of IET resulting from emotional expression in communication, and this is what has the greatest relevance to HRI. 

There is evidence to suggest that one form of IET is social appraisal, whereby individual human judgement is influenced by the perceived judgement of others \cite{parkinson2011interpersonal}. Specifically, it has been demonstrated that judgement of an everyday object differs depending on whether it is presented alongside a smiling or disgusted face \cite{bayliss2007affective} suggesting that emotional expression is a trigger for this form of IET. Another experiment demonstrated that even in a very dangerous situation (a simulated fire) participants surrounded by seemingly calm and unresponsive actors were slower to react than participants that were alone; it has been argued that this could be due to IET effects whereby the participants felt calmer due to the calmness of the actors and also judged the situation to be less dangerous based on their percieved lack of concern (\cite{latane1968group} as discussed in \cite{parkinson2011interpersonal}).

Another evidenced form of IET is emotion contagion, whereby an individual's emotional state changes based on that of their interaction partner; however, this is less well understood and more difficult to experimentally examine than social appraisal effects \cite{parkinson2011interpersonal}. There is a general consensus that most emotion contagion is a form of social mimicry, however, it is disputed whether this is based on physical mimicry, in which the related emotion is generated from mimicking the physical expression; i.e. the individual smiles in response to a smile and hence feels happier \cite{strack1988inhibiting}, or whether expression is less important and actually an individual must understand and perceive the reason for another's emotional expression in order for emotion contagion to occur \cite{tamietto2009unseen}. In contrast to the idea of simple mimicry it should also be noted that emotion contagion has been demonstrated to induce contrasting rather than matching emotions, and that this may be linked to social stature \cite{tiedens2003power}.

Demonstrated examples of emotion contagion highlight the impact it can have both at the individual and group level. For example, listening to neutral information spoken in an emotional way can induce similar emotions in the listener \cite{neumann2000mood}, and emotion contagion in a group can improve attitudes and reduce conflicts resulting in improved task performance \cite{barsade2002ripple}. It has even been demonstrated that emotion contagion can occur through social networks, with Facebook users producing more positive or negative posts when the amount of negative or positive emotional content in their newsfeed was reduced respectively \cite{kramer2014experimental}. Other interesting results include thirsty individuals pouring more or less from a drink jug if exposed to a smiling or frowning face respectively \cite{winkielman2005unconscious} or acceptance of an offer being higher for smiling and lower for frowning proposers compared to those that wore neutral expressions \cite{mussel2013value}. 

As such, whilst the origins and exact mechanisms of IET are not clear, the evidence presented suggests it has significant impact on HHI and can influence an individual's feelings, judgement and behaviour as well as group collaboration and effectiveness. This warrants the study of IET in HRI in order to establish whether the same effects can be observed, and if so whether they can be useful e.g. in improving things like robot effectiveness and acceptance or task performance. Key research questions include whether robot to human IET can occur and how robot emotional expressions affect human judgements and behaviour. Additionally, the number of unconfirmed hypothesise in the psychology literature suggests that HRI experiments dealing with emotion in a very controlled way might be useful as a mechanism for testing theories and contributing to psychological understanding of the role of human emotions. 


\subsection{Human Displays of Emotion}
In order to design emotionally expressive robot behaviours it must first be established how emotion is expressed in humans and identify specific behaviours which might be transferable onto a robot. Specifically the robotic platform to be used for this project is Aldebaran Robotics' humanoid robot NAO\footnote{https://www.aldebaran.com/en/cool-robots/nao}, which means that facial expression cannot be altered and hence lies outside the scope of this project.  As such, the following discussion covers emotional expression through speech and movement only. It is noted that the literature describes facial expression as a complex and important part of affective communication and so future work would be to extend this investigation to include that.

The study of emotion recognition in point light displays generated from dance and acting performances has demonstrated that movement alone can express emotion even if the semantic purpose of that movement is unknown (e.g. \cite{dittrich1996perception}, \cite{pollick2001perceiving}, \cite{atkinson2004emotion}). Laban Movement Analysis (LMA), a multidisciplinary tool for movement analysis considering parameters such as weight, space and time, has become a standard method for parameterising movement in order to further study such effects \cite{lab2011}. As such, instructions on how to perform a certain emotion, as might be explained to dancers or actors, typically utilise LMA (e.g. \cite{newlove1993laban}). Similarly, it is widely accepted that emotion can be portrayed through neutral speech \cite{neumann2000mood} or across languages \cite{scherer2000cross} and similar to LMA, this can be quantitatively described by variation in parameters such as pitch, speed and quality (e.g. \cite{scherer1986vocal}, \cite{cowie2001emotion}). 

The reviewed literature that emotion can theoretically be expressed through any movement or speech, regardless of semantic content. This is particularly relevant for roboticists because it means that emotional expression can be added to communication or task execution without changing the robot's functional behaviour. Furthermore the study of emotion in movement and voice lends credibility to the idea of a parameterisation framework for generating emotional expressions, particularly in a robot such as the NAO with no capability for facial expression. 

\section{IET and Emotional Expression in Robots}
Giving robots the capability to display emotional expressions has been a recurrent theme in social robotics since the field's infancy \cite{breazeal1999build}. Recent demonstrations include robots whose emotional state and hence behaviour is adaptive based on such things at its `personality' \cite{park2009robot} or whether it is winning or losing at a game \cite{tielman2014adaptive}. Whilst adaptive emotional expression is certainly an area of further work for this project, the initial concern is simply the generation of pre-determined emotions which can be displayed through generic functional behaviours through adjusting vocal and movement parameters as discussed above. Examples of a parameter based emotion framework have already been demonstrated (e.g. \cite{masuda2010motion}, \cite{lim2011converting}, \cite{xu2013mood}) and used to successfully generate emotional expressions in a range of robots including the NAO (\cite{lim2011converting} and \cite{xu2013mood}). 

Masuda and Koto's system uses the six main parameters of LMA; space, time, weight, inclination, height and area, which are set based on previous analysis of observed movement emotion classification from a pilot experiment \cite{masuda2009emotion}. Implemented on a humanoid robot the resulting motion had an average emotion recognition rate greater than 60\% \cite{masuda2010motion}. Lim et al.'s framework for adding emotion to gesturing uses only four parameters: speed, intensity, regularity and extent, which are set based on a mapping from the same features measured in an actor's speech sample. Implemented on a NAO the resulting motion had an emotion recognition rate of above 60\% and, when combined with the original speech sample, lead to improved recognition rates for the emotions of happiness and sadness compared to speech alone \cite{lim2011converting}. Xu et al.'s framework uses a combination of general motion and pose parameters (e.g. speed, decay rate, stroke curves) as well as gesture specific ones (e.g. palm up or down). In addition, the head is utilised as an effector which can be set in different poses. Parameter settings were then derived by averaging the results of an experiment in which participants were asked to set them in order to achieve specific emotional expressions on a NAO \cite{xu2013mood}. A later experiment demonstrated that different arousal and valence states can be recognised based on these parameters; however, no results for specific emotion recognition were described \cite{xu2013bodily}.

Considering these three models, Lim et al.'s is the most simple and yet achieved very similar recognition rates to Masuda and Koto's system. In addition the idea of setting gesture parameters based on feature matching to speech samples is attractive for generating natural looking behaviour. How well this feature mapping would work on artificially generated speech as opposed to actor samples is not clear because the effectiveness of the whole system is dependent on how well the provided speech sample portrays emotion (an option in some text to speech engines e.g. CereVoice\footnote{https://www.cereproc.com/en/products/sdk}). However, if the parameter values could be set in a different way, the concept of having a small number of generic parameters that map directly onto both speech and motion could form the basis of a simple yet powerful framework for easily generating complete emotional expressions. Xu et al.'s method of essentially crowd-sourcing parameter settings for different emotions may offer such an option. 

Of these three models only Xu et al. went on to evaluate the impact of their generated emotional expression on HRI, specifically attempting to demonstrate whether robot to human IET occurred, for which they did find some evidence \cite{xu2014robot}. The experimental design of Xu et al.'s study, as well as others concerned with affective communication and HRI, are discussed in the following section.

\section{Experimentally Evaluating the Impact of Affective Communication}

Previous studies documenting the impact of affective robot communication have typically produced only qualitative data. For example, Tielman et al. demonstrated an adaptive emotion model implemented on a NAO used to play a quiz game with children; by using questionnaires they determined that children found emotional expression to be a positive trait for a robot \cite{tielman2014adaptive}. Similarly, a long term study documenting the use of a humanoid game playing robot in an elderly care home found, also by questionnaire, that users rated emotional expression to be one of their favourite robot traits \cite{louie2012playing}. Clearly such qualitative results are important and can offer a valuable insight into HRI, especially surrounding how well `liked' the robot is which is arguably some measure of the robot's effectiveness itself; however, examples from the psychology literature suggest that it should also be possible to produce more quantitative results which demonstrate the measured impact of affective communication, e.g. around the performance of a task \cite{barsade2002ripple} or reaction to an event \cite{latane1968group}.

In a rare example of quantitative study considering robot to human IET, Xu et al. demonstrated that participants performed better in a harder task when working with a robot displaying a negative rather than positive `mood'; they then used this result to argue that emotion contagion had occurred because of a hypothesised psychological phenomenon that humans undertake certain types of task better when in a negative mood \cite{xu2014robot}. Finding further inspiration for evaluation methods and experiment design requires the consideration of HRI studies that do not specifically consider affective communication but do measure the impact of different robot behaviours. Two such studies are outlined below.

Chidambaram et al. used a desert survival HRI task to demonstrate how both vocal and nonverbal robot cues affected their robot's persuasiveness \cite{chidambaram2012designing}. They used a range of conditions considering variation or lack thereof in body movement (proximity, gaze and gesturing) and voice (pitch). Their evaluation measures combined subjective surveys of perceived persuasiveness and objective measures of compliance (i.e. actual persuasiveness). This had the advantage of allowing a valuable comparison between actual and perceived persuasion to be made in the discussion. 

Nakagawa et al. used a monotonous task with a robot companion to determine the effect of robot touch on motivation \cite{nakagawa2011effect}. Participants were asked to undertake a monotonous task for as long as they liked; the base condition had the robot companion only talking to the participant, the two other conditions involved the robot also being passively touched by or actively touching the participant while they worked. The time spent undertaking the task was measured for each condition to give a quantitative measure of the impact. Similar to the work of Chidambaram et al., subjective feedback measures were also used to collect information on the participants' perception but in this case considering robot perception generally rather than specifically its use in motivation. 

In summary there are very few previous HRI studies that deal specifically with the impact of affective communication; however, additional inspiration for quantitative measurements can be drawn from other HRI studies. How best to demonstrate the impact of affective communication is one of the key research questions considered in this project; the aim is to generate insightful data that offers some quantitative measure of impact but also captures more qualitative information to allow for interesting discussion. Considering this aim alongside the example experiments discussed here highlights human/robot task performance, human emotional state and subjective participant opinions as key measures for evaluation.

\chapter{Research Methodology}
[intro paragraph]
\section{Generating Emotional Expressions on NAO}
\subsection{Gesture Modification: SIRE Framework}
Application of Lim's SIRE framework to any gesture
Use of Choregraphe (as opposed to...?)

Pseudo-code for the SIRE framework:
Each gesture consists of moving from base pose to the extended pose and back again. Some of the more complex gestures utilise multiple extended poses... however essentially the same principle applies. For each gesture the following key parameters were defined:
\begin{itemize}
\item pmin : a minimum amplitude version of the gesture described in the Cartesian co-ordinate set $[x,y,z,\alpha,\beta,\gamma]$
\item pmax : maximum amplitude version of the gesture described in the Cartesian co-ordinate set $[x,y,z,\alpha,\beta,\gamma]$
\item tExt : time for extension gesture based on average from an actor video
\item tPos : time extended posture is held based on average from an actor video
\item tRet : time for return gesture based on average from an actor video
\item tMin : minimum time required to execute gesture (for safe operation)
\item rMax : maximum time offset for irregular movements (such that gesture is still clear)
\end{itemize}

\textbf{Speed \& Intensity}
\\Speed of the movement is adjusted by modifying the extension and return times passed to the path planner, tExt and tRet. The maximum times described above are multiplied by $(1-S)$ where S is the speed parameter, i.e. as speed is increased the gesture time is reduced. A maximum operator is used to compare the adjusted time with the specified minimum to ensure the movement is not too fast.

Intensity is applied by further reducing the extension time, $t_{1}$, in the same way as speed, by multiplication with $(1-I)$ where I is the intensity parameter. This gives the appearance of essentially accelerating gesture extension with no change to the return movement. 

\begin{equation}
t_{1} = max[(1-S)*(1-I)*tExt,tmin]
\end{equation}

\begin{equation}
t_{2} = max[(1-S)*tRet,tmin]
\end{equation}

\textbf{Regularity}
\\ Regularity is applied by introducing a temporal delay, $dt$, defined based on $(1-R)$ where R is the regularity parameter, between execution of the left and right arm movements in gestures which utilise both arms. Doing anything with head movements (as in Lim's original)?? Perhaps not as may be confusing given task requires movement copying - explain if this is the case.
\begin{equation}
dt = (1-R)*rMax
\end{equation}

\begin{equation}
t(rightArm) = t(leftArm) + dt
\end{equation}

\textbf{Extent}
\\Extent is applied by adjusting the size of the gesture between the minimum and maximum, by adding a proportion of the difference between them (set by the extent value E) to the minimum. 
\begin{equation}
p = p_{min} + E*(p_{max} - p_{min})
\end{equation}

Positive and negative SIRE values initially set based on general design principles identified by Xu et al \cite{xu2013mood} and specific values found by Lim (cite her thesis); neutral just set as all 0.5 (medium) apart from regularity which is 1.

As interaction activity is to copy gestures for a workout, regularity was not applied as could be confusing; explain this is ok because Lim's thesis showed that SIE was enough to show happiness and sadness. 

Generic design principles identified:

\begin{tabular}{|c|c|c|}
\hline & Positive & Negative \\ 
\hline Speed & high & medium \\ 
\hline Intensity & high & medium \\ 
\hline Regularity & high & high \\ 
\hline Extent & high & medium \\ 
\hline 
\end{tabular} 

Actual initial values used:

\begin{tabular}{|c|c|c|c|}
\hline & Positive & Negative & Neutral \\ 
\hline Speed & 0.8 & 0.1 & 0.5\\ 
\hline Intensity & 0.2 & 0.5 & 0.5 \\ 
\hline Regularity & 1.0 & 1.0 & 1.0 \\ 
\hline Extent & 0.8 & 0.1 & 0.5\\ 
\hline 
\end{tabular} 

\subsection{Speech Generation}
Cerevoice emotion tags

\subsection{Validation of Emotional Expressions}
Recognition experiments

\section{Human Robot Interaction Activity}
Arm exercises, references to relevant papers

\section{Testing for Emotion Contagion}
\subsection{Hypotheses}
\begin{itemize}
	\item[H1] Participants in the happy condition will demonstrate a more positive emotional state, compared to those in the neutral condition, after interacting with the robot
\end{itemize}
\subsection{Measures}

\section{Testing Impact on Motivation/Performance}
\subsection{Hypothesis}
\begin{itemize}
	\item[H1] Participants in the happy condition will carry out the defined monotonous task for longer than those in the neutral condition
\end{itemize}
\subsection{Measures}

\section{Experimental Procedure}
Overview/schedule of experiment as experienced by participants

\chapter{Results}

\chapter{Discussion}

\chapter{Conclusions}

\bibliographystyle{unsrt}
\bibliography{MyReading}

\end{document}