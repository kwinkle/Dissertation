\documentclass[11pt,a4paper]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=3.00cm]{geometry}
\usepackage{helvet} %uarial alt. package for arial font! Try to see if its better
\usepackage{natbib} %https://www.sharelatex.com/learn/Natbib_citation_styles
\renewcommand{\familydefault}{\sfdefault}

\title{Enhancing Social HRI using Affective Communication}
\author{Katie Winkle}
\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}
[intro]
[background/motivation]
[aims/objectives, remember feedback that these can be quite specific now -> high level hypotheses]
[after reading this reader should know premise and context of final experiment so can refer to it before explaining in detail in methodology section]

\chapter{Literature Review}
[intro paragraph]
\section{IET and Emotional Expression in Humans}

\subsection{IET and its Impact on Human Behaviour}
There are different hypotheses concerning the purpose of IET and emotional expression more generally in HHI. A functionalist approach (considering the consequences in order to determine the purpose) suggests that at the dyadic level, emotion expressions help individuals determine other's emotions, beliefs, intentions and orientation towards their relationship; and that evoking emotions in others is associated with behaviours such as avoidance, helping, affiliation and soothing \cite{keltner1999social}. An evolutionary approach (considering development over time and the link to population fitness) suggests that emotional expression evolved from being a physiological response (e.g. scrunching the nose to prevent inhalation of noxious gas) into a form of social communication, which observers evolved an ability to instantly and subconsciously decode in order to obtain information about the expresser and/or their environment \cite{shariff2011emotion}. Regardless of exact purpose, a consistent theme in psychology literature is the importance and subconscious nature of IET resulting from emotional expression in communication, and this is what has the greatest relevance to HRI. 

There is evidence to suggest that one form of IET is social appraisal, whereby individual human judgement is influenced by the perceived judgement of others \cite{parkinson2011interpersonal}. Specifically, it has been demonstrated that judgement of an everyday object differs depending on whether it is presented alongside a smiling or disgusted face \cite{bayliss2007affective} suggesting that emotional expression is a trigger for this form of IET. Another experiment demonstrated that even in a very dangerous situation (a simulated fire) participants surrounded by seemingly calm and unresponsive actors were slower to react than participants that were alone; it has been argued that this could be due to IET effects whereby the participants felt calmer due to the calmness of the actors and also judged the situation to be less dangerous based on their perceived lack of concern (\cite{latane1968group} as discussed in \cite{parkinson2011interpersonal}).

Another evidenced form of IET is emotion contagion, whereby an individual's emotional state changes based on that of their interaction partner; however, this is less well understood and more difficult to experimentally examine than social appraisal effects \cite{parkinson2011interpersonal}. There is a general consensus that most emotion contagion is a form of social mimicry, however, it is disputed whether this is based on physical mimicry, in which the related emotion is generated from mimicking the physical expression; i.e. the individual smiles in response to a smile and hence feels happier \cite{strack1988inhibiting}, or whether expression is less important and actually an individual must understand and perceive the reason for another's emotional expression in order for emotion contagion to occur \cite{tamietto2009unseen}. In contrast to the idea of simple mimicry it should also be noted that emotion contagion has been demonstrated to induce contrasting rather than matching emotions, and that this may be linked to social stature \cite{tiedens2003power}.

Demonstrated examples of emotion contagion highlight the impact it can have both at the individual and group level. For example, listening to neutral information spoken in an emotional way can induce similar emotions in the listener \cite{neumann2000mood}, and emotion contagion in a group can improve attitudes and reduce conflicts resulting in improved task performance \cite{barsade2002ripple}. It has even been demonstrated that emotion contagion can occur through social networks, with Facebook users producing more positive or negative posts when the amount of negative or positive emotional content in their newsfeed was reduced respectively \cite{kramer2014experimental}. Other interesting results include thirsty individuals pouring more or less from a drink jug if exposed to a smiling or frowning face respectively \cite{winkielman2005unconscious} or acceptance of an offer being higher for smiling and lower for frowning proposers compared to those that wore neutral expressions \cite{mussel2013value}. 

As such, whilst the origins and exact mechanisms of IET are not clear, the evidence presented suggests it has significant impact on HHI and can influence an individual's feelings, judgement and behaviour as well as group collaboration and effectiveness. This warrants the study of IET in HRI in order to establish whether the same effects can be observed, and if so whether they can be useful e.g. in improving things like robot effectiveness and acceptance or task performance. Key research questions include whether robot to human IET can occur and how robot emotional expressions affect human judgements and behaviour. Additionally, the number of unconfirmed hypothesise in the psychology literature suggests that HRI experiments dealing with emotion in a very controlled way might be useful as a mechanism for testing theories and contributing to psychological understanding of the role of human emotions. 


\subsection{Human Displays of Emotion}
In order to design emotionally expressive robot behaviours it must first be established how emotion is expressed in humans and identify specific behaviours which might be transferable onto a robot. Specifically the robotic platform to be used for this project is Aldebaran Robotics' humanoid robot NAO\footnote{https://www.aldebaran.com/en/cool-robots/nao}, which means that facial expression cannot be altered and hence lies outside the scope of this project.  As such, the following discussion covers emotional expression through speech and movement only. It is noted that the literature describes facial expression as a complex and important part of affective communication and so future work would be to extend this investigation to include that.

The study of emotion recognition in point light displays generated from dance and acting performances has demonstrated that movement alone can express emotion even if the semantic purpose of that movement is unknown (e.g. \cite{dittrich1996perception}, \cite{pollick2001perceiving}, \cite{atkinson2004emotion}). Laban Movement Analysis (LMA), a multidisciplinary tool for movement analysis considering parameters such as weight, space and time, has become a standard method for parameterising movement in order to further study such effects \cite{lab2011}. As such, instructions on how to perform a certain emotion, as might be explained to dancers or actors, typically utilise LMA (e.g. \cite{newlove1993laban}). Similarly, it is widely accepted that emotion can be portrayed through neutral speech \cite{neumann2000mood} or across languages \cite{scherer2000cross} and similar to LMA, this can be quantitatively described by variation in parameters such as pitch, speed and quality (e.g. \cite{scherer1986vocal}, \cite{cowie2001emotion}). 

The reviewed literature therefore suggets that emotion can theoretically be expressed through any movement or speech, regardless of semantic content. This is particularly relevant for roboticists because it means that emotional expression can be added to communication or task execution without changing the robot's functional behaviour. Furthermore the study of emotion in movement and voice lends credibility to the idea of a parameterisation framework for generating emotional expressions, particularly in a robot such as the NAO with no capability for facial expression. 

\section{IET and Emotional Expression in Robots}
Giving robots the capability to display emotional expressions has been a recurrent theme in social robotics since the field's infancy \cite{breazeal1999build}. Recent demonstrations include robots whose emotional state and hence behaviour is adaptive based on such things at its `personality' \cite{park2009robot} or whether it is winning or losing at a game \cite{tielman2014adaptive}. Whilst adaptive emotional expression is certainly an area of further work for this project, the initial concern is simply the generation of pre-determined emotions which can be displayed through generic functional behaviours through adjusting vocal and movement parameters as discussed above. Examples of a parameter based emotion framework have already been demonstrated (e.g. \cite{masuda2010motion}, \cite{lim2011converting}, \cite{xu2013mood}) and used to successfully generate emotional expressions in a range of robots including the NAO (\cite{lim2011converting} and \cite{xu2013mood}). 

Masuda and Koto's system uses the six main parameters of LMA; space, time, weight, inclination, height and area, which are set based on previous analysis of observed movement emotion classification from a pilot experiment \cite{masuda2009emotion}. Implemented on a humanoid robot the resulting motion had an average emotion recognition rate greater than 60\% \cite{masuda2010motion}. Lim et al.'s framework for adding emotion to gesturing uses only four parameters: speed, intensity, regularity and extent, which are set based on a mapping from the same features measured in an actor's speech sample. Implemented on a NAO the resulting motion had an emotion recognition rate of above 60\% and, when combined with the original speech sample, lead to improved recognition rates for the emotions of happiness and sadness compared to speech alone \cite{lim2011converting}. Xu et al.'s framework uses a combination of general motion and pose parameters (e.g. speed, decay rate, stroke curves) as well as gesture specific ones (e.g. palm up or down). In addition, the head is utilised as an effector which can be set in different poses. Parameter settings were then derived by averaging the results of an experiment in which participants were asked to set them in order to achieve specific emotional expressions on a NAO \cite{xu2013mood}. A later experiment demonstrated that different arousal and valence states can be recognised based on these parameters; however, no results for specific emotion recognition were described \cite{xu2013bodily}.

Considering these three models, Lim et al.'s is the most simple and yet achieved very similar recognition rates to Masuda and Koto's system. In addition the idea of setting gesture parameters based on feature matching to speech samples is attractive for generating natural looking behaviour. How well this feature mapping would work on artificially generated speech as opposed to actor samples is not clear because the effectiveness of the whole system is dependent on how well the provided speech sample portrays emotion (an option in some text to speech engines e.g. CereVoice\footnote{https://www.cereproc.com/en/products/sdk}). However, if the parameter values could be set in a different way, the concept of having a small number of generic parameters that map directly onto both speech and motion could form the basis of a simple yet powerful framework for easily generating complete emotional expressions. Xu et al.'s method of essentially crowd-sourcing parameter settings for different emotions may offer such an option. 

Of these three models only Xu et al. went on to evaluate the impact of their generated emotional expression on HRI, specifically attempting to demonstrate whether robot to human IET occurred, for which they did find some evidence \cite{xu2014robot}. The experimental design of Xu et al.'s study, as well as others concerned with affective communication and HRI, are discussed in the following section.

\section{Experimentally Evaluating the Impact of Robot Affect}

Previous studies documenting the impact of affective robot behaviour have typically produced only qualitative data. For example, Tielman et al. demonstrated an adaptive emotion model implemented on a NAO used to play a quiz game with children; by using questionnaires they determined that children found emotional expression to be a positive trait for a robot \cite{tielman2014adaptive}. Similarly, a long term study documenting the use of a humanoid game playing robot in an elderly care home found, also by questionnaire, that users rated emotional expression to be one of their favourite robot traits \cite{louie2012playing}. Clearly such qualitative results are important and can offer a valuable insight into HRI, especially surrounding how well `liked' the robot is which is arguably some measure of the robot's effectiveness itself; however, examples from the psychology literature suggest that it should also be possible to produce more quantitative results which demonstrate the measured impact of affective communication, e.g. around the performance of a task \cite{barsade2002ripple} or reaction to an event \cite{latane1968group}.

In a rare example of quantitative study considering robot to human IET, Xu et al. demonstrated that participants performed better in a harder task when working with a robot displaying a negative rather than positive `mood'; they then used this result to argue that emotion contagion had occurred because of a hypothesised psychological phenomenon that humans undertake certain types of task better when in a negative mood \cite{xu2014robot}. Finding further inspiration for evaluation methods and experiment design requires the consideration of HRI studies that do not specifically consider affective communication but do measure the impact of different robot behaviours. Three such studies are outlined below.

Chidambaram et al. used a desert survival HRI task to demonstrate how both vocal and nonverbal robot cues affected their robot's persuasiveness \cite{chidambaram2012designing}. They used a range of conditions considering variation or lack thereof in body movement (proximity, gaze and gesturing) and voice (pitch). Their evaluation measures combined subjective surveys of perceived persuasiveness and objective measures of compliance (i.e. actual persuasiveness). This had the advantage of allowing a valuable comparison between actual and perceived persuasion to be made in the discussion. 

Nakagawa et al. used a monotonous task with a robot companion to determine the effect of robot touch on motivation \cite{nakagawa2011effect}. Participants were asked to undertake a monotonous task for as long as they liked; the base condition had the robot companion only talking to the participant, the two other conditions involved the robot also being passively touched by or actively touching the participant while they worked. The time spent undertaking the task was measured for each condition to give a quantitative measure of the impact. Similar to the work of Chidambaram et al., subjective feedback measures were also used to collect information on the participants' perception but in this case considering robot perception generally rather than specifically its use in motivation. 

Goetz and Kiesler investigated differences in compliance with an exercise regime delivered by a serious or playful robot \cite{goetz2002cooperation}. The serious robot talked about health issues relating to the exercise whereas the playful robot made jokes and treated the exercises as fun. After being led through some mandatory exercises participants were asked to make up their own routine and do it for as long as they could; time spent on this routine was then recorded as quantitative measure of compliance. Participants were also asked to rate their impressions of the robot's personality and intellect in order to investigate the differences in robot perception across the two conditions.

In summary there are very few previous HRI studies that deal specifically with the impact of affective communication; however, additional inspiration for quantitative measurements can be drawn from other HRI studies. How best to demonstrate the impact of affective communication is one of the key research questions considered in this project; the aim is to generate insightful data that offers some quantitative measure of impact but also captures more qualitative information to allow for interesting discussion. Considering this aim alongside the example experiments discussed here highlights human/robot task performance, human emotional state and subjective participant opinions as key measures for evaluation.

\chapter{Research Methodology}
[intro paragraph]
\section{Generating Emotional Expressions on NAO}
[Intro paragraph on wanting to be able to make functional gestures emotional rather than expressing emotion through specific emotive movements]

\begin{itemize}
	\item nao limitations - no facial expression so had to be done via movement and speech only
	\item previous emotional display on NAO focused on gesture modification - Lim and Xu work
	
\end{itemize}

\subsection{Gesture Modification: SIRE Framework}
Application of Lim's SIRE framework to any gesture

Pseudo-code for the SIRE framework:
Each gesture consists of moving from base pose to the extended pose and back again. For each gesture the following key parameters were defined:
\begin{itemize}
\item $pMin$ : a minimum amplitude version of the gesture described in the Cartesian co-ordinate set $[x,y,z,\alpha,\beta,\gamma]$
\item $pMax$ : maximum amplitude version of the gesture described in the Cartesian co-ordinate set $[x,y,z,\alpha,\beta,\gamma]$
\item $tExt$ : time for extension gesture based on average from an actor video
\item $tPos$ : time extended posture is held based on average from an actor video
\item $tRet$ : time for return gesture based on average from an actor video
\item $tMin$ : minimum time required to execute gesture (for safe operation)
\item $rMax$ : maximum time offset for irregular movements (such that gesture is still clear)
\end{itemize}

\subsubsection{Speed \& Intensity}
Speed of the movement is adjusted by modifying the extension and return times passed to the path planner, tExt and tRet. The maximum times described above are multiplied by $(1-S)$ where $S$ is the speed parameter, i.e. as speed is increased the gesture time is reduced. A maximum operator is used to compare the adjusted time with the specified minimum to ensure the movement is not too fast.

Intensity is applied by further reducing the extension time, $t_{1}$, in the same way as speed, by multiplication with $(1-I)$ where $I$ is the intensity parameter. This gives the appearance of essentially accelerating gesture extension with no change to the return movement. 

\begin{equation}
t_{1} = max[(1-S)*(1-I)*tExt,tmin]
\end{equation}

\begin{equation}
t_{2} = max[(1-S)*tRet,tmin]
\end{equation}

\subsubsection{Regularity}
Regularity is applied by introducing a temporal delay, $dt$, defined based on $(1-R)$ where $R$ is the regularity parameter, between execution of the left and right arm movements in gestures which utilise both arms. Lim et al. \cite{lim2011converting} also used the $R$ value to define side-to-side head movements, this was not implemented here however due to the imitation based exercise context of the main HRI experiment. 
\begin{equation}
dt = (1-R)*rMax
\end{equation}

\begin{equation}
t(rightArm) = t(leftArm) + dt
\end{equation}

\subsubsection{Extent}
Extent is applied by adjusting the size of the gesture between the minimum and maximum, by adding a proportion of the difference between them (set by the extent value E) to the minimum. 
\begin{equation}
p = p_{min} + E*(p_{max} - p_{min})
\end{equation}

\subsubsection{Setting Initial SIRE Values}
Lim et al. \cite{lim2011converting} originally set gesture SIRE values based on those extracted from actor speech samples; however whether this method could work for artificial speech from a text to speech generator requires further investigation outside the scope of this project. Instead, for this project, initial SIRE values were set based on some generic principles identified by Xu et al. in conjunction with specific numeric values identified by Lim [CITE HER THESIS HERE]. Generic design principles identified based on the work of Xu et al. and Lim et al. are given in Table [REF]; specific values used for initial testing are listed in table [REF]. 

Generic design principles identified:

\begin{tabular}{|c|c|c|}
\hline & Positive & Negative \\ 
\hline Speed & high & low \\ 
\hline Intensity & high & low \\ 
\hline Regularity & high & high \\ 
\hline Extent & high & low \\ 
\hline 
\end{tabular} 

Actual initial values used:

\begin{tabular}{|c|c|c|c|}
\hline & Positive & Negative & Neutral \\ 
\hline Speed & 0.8 & 0.1 & 0.4\\ 
\hline Intensity & 0.8 & 0.1 & 0.4 \\ 
\hline Regularity & 1.0 & 1.0 & 1.0 \\ 
\hline Extent & 0.8 & 0.1 & 0.4\\ 
\hline 
\end{tabular} 

Regularity was kept at the maximum level across all conditions for two key reasons. Firstly, given the imitation based nature of the main HRI experiment, introducing any lag between arm movements or additional head wobble could be confusing to participants as it would not be part of the exercise routine and would diverge from the spoken instructions. Secondly, Lim demonstrated that the $S, I $ and $E$ paramaters were sufficient to portray happiness and sadness, the two emotional conditions considered in this project, and specified that regularity is more associated with emotions such as fear and excitement [CITE THESIS]. 

\subsection{Speech Generation}
Cereproc/Cerevoice; emotion tags

\subsection{Choregraphe}
Controlling NAO - both voice and speech, how timing was set etc.

\section{Testing \& Validation of Emotion Expression}
Three emotion recognition experiments were carried out in order to test the initial SIRE values described above. The first two tested emotion recognition in voice only and gesture only whereas the third tested the combined voice and gesture as would be used in the final experiment. Testing each modality independently at first allowed for validation of the Cereproc emotional tags and the SIRE framework for gesturing, and testing the combined performance allowed for validation of the final system as intended for use in the main HRI experiment. 

Details - participant numbers, same participants for voice and gesture only, new for combined. Online video survey setup, randomised order for each participant, number of samples... Percentage recognition results for each condition are listed in Table [REF]. 

\begin{tabular}{|c|c|}
	\hline Condition & Recognition\\ 
	\hline Voice Only & 60.4\% \\ 
	\hline Gesture Only & 68.2\% \\ 
	\hline Voice + Gesture & 65.0\% \\ 
	\hline 
\end{tabular} 

Percentage recognition very similar to what Lim achieved and therefore further supports use of something like Cereproc rather than hand manipulated voice. 

Give pretesting results here including the extremity thing.

Chi-squared testing... all emotions were clearly distinguishable across all conditions. Overall recognition was not significantly different across the conditions. Additional analysis showed that the number of extreme emotion choices (i.e. very happy or very sad) significantly increased between the voice only and voice plus gesture conditions; but not between the gesture only and voice plus gesture conditions. This is discussed further in Section [REF DISCUSSION SECTION]. 

\section{Human Robot Interaction Experiment Design}
\subsection{Interaction Activity \& Context}

An imitation based robot-led exercise session was chosen to provide a contextualised HRI activity for testing the impact of affective robot communication. An imitation based activity was preferred as it inherently encourages the participant to focus on the robot's gestures; Xu et al. used a gesture imitation game for this reason \cite{xu2014robot} ... [however there is some psychological evidence that...emotional impact of winning/losing] The use of robots as an exercise instructor has been extensively investigated [REVIEW PAPER or SOME REFS], specifically relevant to this project is Mataric's work on a robotic arm exercise instructor for the elderly [REF] which essentially results in the same robot interaction as Xu et al.'s setup but contextualised as a workout rather than a game. [maybe something about therefore allowing for comparison to Xu's results too? - also talk about how the specific gestures and spoken commands used matched Xu's perfectly for maximum comparison ability]

[gesture round image showing spoken instructions]

[mandatory versus voluntary round image]

Other work by Mataric investigated the effect of robot personality [was it robot personality or human personality? Check this i think it was human personality actually...] on participant's motivation to do repetitive and monotonous stroke exercises [REF]. Participants were asked to do as much of each exercise as they felt necessary and this number was measured as a quantitative measure of motivation; similar techniques were also employed by .... [REFs]. Therefore it was decided that the robot-led exercise session would consist of one mandatory gesture set (in order to allow for contagion to possibly occur) followed by a series of voluntary rounds which could provide a quantitative measure of motivation. [this is also inline with those other papers from lit review ie touch and persuasion papers]

[maybe a new section called implementation or method or materials or something? all the gory details in terms of gesture set randomisation, round randomisation, order of survey questions (word completion test, q's on enjoyment/good exercise instructor, bored/tiredness, manipulation and own emotion check) etc etc...]

\subsection{Hypotheses}
\begin{itemize}
	\item[H1] Participants in the valenced conditions will demonstrate an emotional state of matching valence after interacting with the robot, (i.e. emotion contagion will occur)
	\item[H2] Participants will recognise the intended emotional state of the robot when interacting with it
	\item[H3] Emotion expression will have an impact on human motivation and enjoyment 
	\item[H4] Emotion expression will have an impact on robot perception
\end{itemize}

[Reference all the literature as to why these are expected and any interesting previous results]

\subsection{Measures}

Something about quantitative measure already described above but also wanted to look at impact on perception and provide opportunity for qualitative feedback...

\begin{tabular}{|l|p{7cm}|p{3.8cm}|}
\hline Feature of Interest & Related Measures & Relevant Hypotheses \\ 
\hline Emotion Contagion & Word completion task \newline Emotional state (of robot and self) & H1 \newline H1 \& H2 \\ 
\hline Impact on HRI & Number of exercise rounds voluntarily completed \newline Enjoyment Likert scale \newline Reason for stopping Likert scale \newline Perceived usefulness Likert scale \newline Selected Godspeed Questions \newline Open feedback/comments &  H3 \newline \newline H3 \newline H3 \newline H4 \newline H4 \newline H4 \\
\hline 
\end{tabular} 

\subsubsection{Word Completion Test}
Word completion tests require the participant being asked to complete a word stem (a word with one or more missing letters) which has multiple potential completions with the first letter that comes to mind. Such tests have been used in HRI studies to measure death thought accessibility \cite{koschate2016overcoming} [more REFs] but also in human psychology studies in order to measure emotional state e.g. \cite{dewall2007terror}. [COMPARISON TO OTHER MOOD MEASURE METHODS]

A word stem list was generated using the [HOW TO REF THE ANEW THING?]. Firstly all words with a valence greater than 7 (positive), less than 3 (negative) and exactly 5 (neutral) were identified. Possible word stems and alternative completions were then considered for each of the valenced words to find word stems which had one valenced completion (positive or negative only) and one or more neutral completions. For the neutral words word stems with only neutral completions were selected. Finally the word frequencies of these alternative completions were compared and all word stems with a difference greater than 1 in any completion were discarded. 12 neutral, 6 positive and 6 negative words were selected from the remaining pool to generate an initial word list. 

A pretest [details here e.g. N = 8?] was then carried out  to examine completion frequencies of the valenced words in a relatively neutral context. Valenced word stems which were completed by more than 50\% of participants were discarded and replaced with others from the pool described above. The final word stem list and possible completions is given below [or in appendix?] [Something about how proper validation would work and that this is further work to make the tool a more validated measure etc] 

\subsection{Experimental Procedure}
A total of 62 participants (18 male and 44 female) aged between 21 and 60 (Mean = 31.7 SD = 8.72) were recruited for the experiment and randomly assigned to one of the three conditions resulting in 21 subjects in the happy and neutral conditions and 20 subjects in the sad condition. The quantitative data for one subject in the happy condition was discarded due to a fire alarm disrupting the experiment. Participants were offered a $£$5 Amazon voucher as compensation for taking part in the experiment. 

Participants were first asked to read through an experiment information sheet which stated that the NAO was programmed to guide them through an arm exercise session typically used by older adults and the disabled to keep fit. It was explained that each round of exercise consisted of 4 gestures and that the first round of exercise was mandatory, but after that NAO would ask the participant if they wanted to continue or stop and that they could stop exercising whenever they wanted to; this was highlighted in bold text. 

A demo was then given to show the participants what to expect and how to safely interact with the robot in terms of touching the head or foot to indicate they wanted to continue or end the exercise session respectively. At this point it was verbally pointed out again to the participants that all exercise rounds after the first were voluntary and they could stop at any time. The experimenter than launched the main experiment script and left the room once the robot was seen to be working correctly, returning when the exercise session was complete, either because the participant chose to end the session or the maximum number of rounds was reached.

[robot programme diagram from previous, unless put in earlier in method/details etc]

Once the exercise session was complete participants were asked to complete an online survey containing the word completion task, all Likert and Godspeed questions and robot/self emotional state questions. Participants were then asked to read a debrief sheet which explained the underlying study of affective communication and its impact on HRI with a note not to discuss this with any other potential participants until after they had also completed the experiment. Participants were then given a final chance to ask any questions about or discuss the research further with a note made of any additional qualitative feedback. 

\chapter{Results}
Chi squared for word completion because ... and list results

Anova for voluntary rounds, emotional robot state (is it worth doing that in chi squared to match sire pre-test though?), self state and all likert q's because... and list results

Qualitative data from open ended question at end - look at use of adjectives across conditions, note people saying emotion questions were difficult as they 'knew it couldn't have emotions' 

\chapter{Discussion}

Not sure where this fits but definitely want some discussion on people's expectations and how this changes their judgements i.e. the robot must be neutral because it is a robot and robots dont have emotions (qualitative data which is also discussed in 'how robots should give advice' paper as a reason for the same behaviour done by a robot and human leading to different responses). 

\section{Emotion Recognition in Robot Gesture and Voice}
\begin{itemize}
	\item perceived extremity increased by use of gestures - relevant to telepresence etc
	\item why was emotion recognised correctly in pre test but not exercise experiment? Relative difference? How does this contrast to Lim's result? Can offer quite a lot of discussion on this
\end{itemize}

\section{Robot to Human Emotion Contagion}

\section{Impact of Affective Communication on HRI}
Perception and motivation - liket scale Q's and voluntary rounds completed

\chapter{Conclusions}

Perhaps robot needs to be more explicit about it's emotional state in order for emotion recognition to occur, at least if the situation is one in which emotions wouldn't really be expected. 

\section{Further Work}

A multiple interaction experiment whereby the participant is going to see the different emotional states of the robot. 

An experiment where the robot is explicit about its emotional state to see impact on persuasiveness and contagion - could almost do exactly the same experiment but with explicit emotional discussion at start or some context for that or something.

\bibliographystyle{unsrt}
\bibliography{MyReading}

\end{document}